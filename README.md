# Study Assistant - 100% Open-Source AI Learning Tool

**ğŸ‰ NO API KEYS â€¢ NO COSTS â€¢ 100% PRIVACY â€¢ RUNS LOCALLY**

A high-performance RAG-based study assistant that runs entirely on your machine using open-source models. Ingests lecture PDFs and audio/video content to generate summaries, flashcards, and quizzes.

## âœ¨ Key Features

### ğŸ”’ Privacy-First
- **100% Local**: All processing happens on your machine
- **No API Calls**: Zero data sent to external servers
- **Offline Capable**: Works without internet (after model download)
- **No Subscriptions**: Completely free to use

### ğŸ“š Content Processing
- **Multi-format Input**: PDF documents and audio/video files
- **Intelligent Processing**: PaddleOCR, Whisper ASR, semantic chunking
- **RAG Pipeline**: Hybrid retrieval (vector + BM25) with reranking
- **Smart Caching**: Process documents once, reuse embeddings (58% faster)

### ğŸ“ Content Generation
- **Multi-scale summaries** (sentence, paragraph, section)
- **Flashcards** (definition, concept, cloze)
- **Quiz questions** (MCQ, short-answer, numerical)
- **Export Formats**: Anki (.apkg), CSV

### ğŸŒ MCP Server (Production-Ready)
- **RESTful API**: Upload documents, generate study materials
- **Web Frontend**: Simple drag-and-drop interface
- **Batch Processing**: Generate all 3 types in one request
- **Session Management**: Automatic caching and deduplication
- **Modular Design**: Easy to extend with new models and request types

### ğŸ§  Advanced Features
- **Model Finetuning**: LoRA/QLoRA finetuning optimized for 4GB GPU
- **Hyperparameter Tuning**: Grid search and Bayesian optimization (Optuna)
- **Prompting Strategies**: Base, system, one-shot, and few-shot prompting
- **Quantitative Evaluation**: ROUGE, BERTScore, coverage, factuality metrics
- **Web Search Integration**: DuckDuckGo search for question enrichment (optional)

### ğŸš€ Technology Stack (All Open-Source)
- **LLM**: llama-cpp-python (Mistral, Llama, Phi-3, Gemma)
- **Embeddings**: sentence-transformers (all-MiniLM, all-mpnet)
- **OCR**: PaddleOCR + Tesseract
- **ASR**: OpenAI Whisper (open-source)
- **Vector DB**: FAISS
- **Training**: PEFT (LoRA), BitsAndBytes (4-bit quantization)
- **Optimization**: Optuna (Bayesian hyperparameter search)
- **Evaluation**: ROUGE, BERTScore, NLI (DeBERTa)

## Architecture

```
Input â†’ Preprocessing â†’ Chunking â†’ Embeddings â†’ Vector Store
                                                      â†“
                                                  Retrieval (Hybrid: Vector + BM25)
                                                      â†“
                                              Reranking (Cross-Encoder)
                                                      â†“
                                    Prompting Strategy (Base/System/One-Shot/Few-Shot)
                                                      â†“
                                                  LLM Generation
                                                      â†“
                                    Optional: Web Search Enrichment
                                                      â†“
                                                    Output
```

### Training & Evaluation Pipeline

```
Training Data â†’ Finetuning (LoRA/QLoRA) â†’ Improved Model
                     â†“
          Hyperparameter Search (Optuna)
                     â†“
              Best Configuration
                     â†“
         Evaluation (ROUGE, BERTScore, etc.)
                     â†“
          Improvement Report (JSON)
```

## ğŸš€ Quick Start

### Option 1: MCP Server (Recommended - Web Interface)

```bash
# 1. Setup (one-time)
./setup_mcp_server.sh

# 2. Start the MCP server
./start_mcp_server.sh

# 3. Start the frontend (in new terminal)
./start_frontend.sh

# 4. Open browser to http://localhost:8080
# Upload documents, generate study materials!
```

### Option 2: Python API (For Developers)

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Download model (Mistral-7B)
mkdir -p models
pip install huggingface-hub
huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF \
  mistral-7b-instruct-v0.2.Q4_K_M.gguf \
  --local-dir models/ --local-dir-use-symlinks False

# 3. Use the Python API (see below)
```

**See [SETUP_GUIDE.md](SETUP_GUIDE.md) for detailed installation instructions.**

## ğŸ“– Usage

### Web Interface (Easiest)

1. Start the server: `./start_mcp_server.sh`
2. Start the frontend: `./start_frontend.sh`
3. Open http://localhost:8080
4. Upload PDF or audio file
5. Select mode (Summary, Flashcards, Quiz)
6. Click "Generate" and wait for results

### Python API

```python
from src.pipeline import StudyAssistantPipeline

# Initialize pipeline
pipeline = StudyAssistantPipeline()

# Process documents
pipeline.ingest_pdf("lecture.pdf")
pipeline.ingest_audio("lecture.mp3")

# Generate study materials
summaries = pipeline.generate_summaries()
flashcards = pipeline.generate_flashcards()
quizzes = pipeline.generate_quizzes()

# Export to Anki
pipeline.export_anki("output.apkg")
```

### REST API

```bash
# Upload file
curl -X POST http://localhost:5000/upload -F "file=@lecture.pdf"

# Generate summary
curl -X POST http://localhost:5000/process \
  -H "Content-Type: application/json" \
  -d '{"file_id": "...", "request_type": "summary", "model": "default"}'

# Batch process (all 3 types)
curl -X POST http://localhost:5000/batch-process \
  -H "Content-Type: application/json" \
  -d '{"file_id": "...", "requests": [...]}'
```

**See [MCP_SERVER.md](MCP_SERVER.md) for complete API documentation.**

### Advanced: Model Finetuning

```bash
# Finetune for better quality (uses 4GB GPU with QLoRA)
make train-summary
make train-flashcard
make train-quiz

# Hyperparameter tuning
make tune-bayesian

# Evaluate improvements
make evaluate-improvement
```

## Project Structure

```
study-assistant/
â”œâ”€â”€ config/              # Configuration files
â”‚   â”œâ”€â”€ config.yaml     # Main configuration
â”‚   â””â”€â”€ prompts/        # Custom prompt templates
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/      # PDF & audio input processing
â”‚   â”œâ”€â”€ preprocessing/  # OCR, ASR, cleaning
â”‚   â”œâ”€â”€ representation/ # Chunking, embeddings, vector store
â”‚   â”œâ”€â”€ retrieval/      # Hybrid retrieval & reranking
â”‚   â”‚   â””â”€â”€ websearch/  # Web search integration (NEW)
â”‚   â”œâ”€â”€ generation/     # Summary, flashcard, quiz generation
â”‚   â”‚   â””â”€â”€ prompting/  # Prompting strategies (NEW)
â”‚   â”œâ”€â”€ training/       # Finetuning & hyperparameter tuning (NEW)
â”‚   â”œâ”€â”€ evaluation/     # Metrics & validation (ENHANCED)
â”‚   â”œâ”€â”€ export/         # Anki, CSV exporters
â”‚   â””â”€â”€ pipeline.py     # Main orchestration
â”œâ”€â”€ scripts/            # Utility scripts (NEW)
â”‚   â”œâ”€â”€ preprocess_sample_data.py
â”‚   â””â”€â”€ test_from_preprocessed.py
â”œâ”€â”€ tests/              # Unit and integration tests
â”œâ”€â”€ data/               # Sample data and outputs
â”‚   â”œâ”€â”€ training/       # Training data for finetuning
â”‚   â””â”€â”€ preprocessed/   # Cached OCR/ASR results
â””â”€â”€ results/            # Training and evaluation results
    â”œâ”€â”€ models/         # Finetuned models
    â”œâ”€â”€ metrics/        # Evaluation metrics
    â””â”€â”€ hparams/        # Hyperparameter search results
```

## ğŸ“– Documentation

- **[SETUP_GUIDE.md](SETUP_GUIDE.md)** - Complete setup from scratch to running
- **[MCP_SERVER.md](MCP_SERVER.md)** - MCP server API and web interface
- **[PROJECT_SPECIFICATION.md](PROJECT_SPECIFICATION.md)** - Pipeline architecture and technical details
- **[PROJECT_STATUS.md](PROJECT_STATUS.md)** - Implementation status and recent fixes

## ğŸ’¾ System Requirements

**Minimum** (fast, lower quality):
- RAM: 4GB
- Storage: ~3GB
- CPU: Any modern processor

**Recommended** (balanced):
- RAM: 8GB
- Storage: ~6GB
- CPU: 4+ cores
- GPU: Optional (CUDA for faster inference)

**High Quality**:
- RAM: 16GB
- Storage: ~10GB
- GPU: NVIDIA GPU with 8GB+ VRAM

## ğŸ¯ Development Status

âœ… **Phase 1 Complete**: Core RAG Pipeline
- PDF/Audio ingestion with OCR and ASR
- Hybrid retrieval (vector + BM25) with reranking
- Content generation (summaries, flashcards, quizzes)
- Export to Anki and CSV
- **100% open-source (no API keys required)**

âœ… **Phase 2 Complete**: Advanced Features
- Model finetuning with LoRA/QLoRA (4GB GPU optimized)
- Hyperparameter tuning (grid search + Bayesian optimization)
- Prompting strategies (base, system, one-shot, few-shot)
- Quantitative evaluation metrics (ROUGE, BERTScore, factuality)
- Web search integration for question enrichment

âœ… **Phase 3 Complete**: Production MCP Server (Nov 2025)
- RESTful API with 6 endpoints
- Web frontend with drag-and-drop upload
- Session management and caching (58% faster)
- Batch processing support
- Fixed hallucinations (content now based on actual documents)
- Fixed redundant ASR/OCR processing
- Smart chunking for audio transcripts

**See [PROJECT_STATUS.md](PROJECT_STATUS.md) for detailed status and recent fixes.**

## ğŸ¤ Contributing

Contributions welcome! This project is 100% open-source and community-driven.

## ğŸ“„ License

MIT

---

**Made with â¤ï¸ using only open-source tools. No API keys, no costs, no compromises.**

