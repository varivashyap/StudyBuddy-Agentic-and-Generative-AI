Good morning, welcome to 006, Introduction to Algorithms, lecture 2. I am Eric DeMain and I love algorithms. Are you with me? Today, we're not doing algorithms. We're doing data structures. It's OK. There's lots of algorithms in each data structure. It's like multiple algorithms for free. We're going to talk about sequences and sets and linked lists and dynamic arrays. Fairly simple data structures today. This is the beginning of several data structures we'll be talking about in the next few lectures. But before we actually start with one, let me tell you, slash remind you of the idea, the difference between an interface, which you might call an API, if you're a programmer, or an ADT, if you're an ancient algorithms person like me, versus a data structure. These are useful distinctions. The idea is that an interface says what you want to do, a data structure says how you do it. So you might call this a specification. And in the context of data structures, we're trying to store some data. So the interface will specify what data you can store, whereas the data structure will give you an actual representation and tell you how to store it. This is pretty boring. Just storing data is really easy. You just throw it in a file or something. What makes it interesting is having operations on that data. So in the interface, you specify what the operations do. What operations are supported. And in some sense, what they mean. And the data structure actually gives you algorithms, that's where the algorithms come in, for how to support those operations. So in this class, we're going to focus on two main interfaces, and various special cases of them. So the idea is to separate what you want to do versus how to do it, because you can think of this as the problem statement. So yes, or last class, Jason talked about problems and defined what a problem was, versus algorithmic solutions to the problem. And this is the analogous notion for data structures, where we want to maintain some data according to various operations. So the same problem can be solved by many different data structures. And we're going to see that. And different data structures are going to have different advantages. They might support some operations faster than others, and depending on what you actually use those data structures for, you choose the right data structure. But you can maintain the same interface. We're going to think about two data structures, one is called a set, and one is called a sequence. These are highly loaded terms, set means something to a mathematician. It means something else to a Python programmer, sequence similarly. I guess there's not a Python sequence data type built in. The idea is we want to store n things. The things will be fairly arbitrary. Think of them as integers or strings. And on the one hand, we care about their values, and maybe we want to maintain them in sorted order, and be able to search for a given value, which we'll call a key. And on the other hand, we care about representing a particular sequence that we care about. Maybe we want to represent the numbers 5 to 9, 7 in that order. And store that in Python, you could store that in a list, for example. And it will keep track of that order. And this is the first item, the second item, the last item. Today, we're going to be focusing on this sequence data structure, although at the end, I'll mention the interface for sets, but we're going to be actually solving sequences today. And in the next several lectures, we'll be bouncing back and forth between these. They're closely related, pretty abstract at the moment. On the other hand, we're going to have two main, let's call them data structure tools or approaches. One is arrays. And the other is pointers, pointer-based or linked data structures. You may have seen these. They're used a lot in programming, of course. But we're going to see both of these today. I'll come back to this highlight in a moment. Let's jump into the sequence interface, which I conveniently have part of here. There's a few different levels of sequences that we might care about. I'm going to start with the static sequence interface. This is where the number of items doesn't change, though the actual items might. So here we have n items. I'm going to label them x0 to xn minus 1 as in Python. So the number of items is n. And the operations I want to support are build, length, iteration, get, and set. So what do these do? Build is how you get started to build a data structure in this interface. You call build of x. Exactly how you specify x isn't too important. But the idea is I give you some items in some order. In Python, this would be an iterable. I'm going to want to also know its length. And I want to make a new data structure of size n, a new static sequence of size n, that has those items in that order. So that's how you build one of these. Because somehow we have to specify n to this data structure. Because n is not going to be allowed to change. I'm going to give you a length method. These are methods are the object-oriented way of thinking of operations that your interface supports. So length will just return this fixed value n. Itter sequence, this is the sense in which we want to maintain the order. I want to be able to output x0 through xn minus 1 in the sequence order. In that specified order that they were built in, or that it was changed to. So this is going to iterate through all the items. So it's going to take at least linear time to output that. But more interesting is we can dynamically access anywhere in the middle of this sequence. We can get the i-th item xi given the value i. And we can change xi to a given new item. So that's called getat and setat. Pretty straightforward. This should remind you very closely of something data structure. So this is an interface. This is something I might want to solve. But what is the obvious data structure that solves this problem? A list. In Python, it's called a list. I prefer to call it an array. But to each their own, we're going to use lists. List could mean many things. But the solution to this interface problem, the natural solution. Is what I'll call a static array. Jason mentioned these in lecture one. It's a little tricky because there are no static arrays in Python. There are only dynamic arrays, which is something we will get to. But I want to talk about what is a static array really. And this relates to our model of computation. Jason also talked about, which we call the word RAM. Remember. So the idea in the word RAM is that your memory is an array of w-bit words. This is a bit circular. I'm going to define an array in terms of the word RAM, which is defined in terms of arrays. But I think you know the idea. So we have a big memory, which goes off to infinity maybe. It's divided into words. Each word here is w-bits long. This is word zero, word one, word two. And you can access this array randomly. Random access memory. So I can give you the number five. And get zero, one, two, three, four, five. The fifth word in this RAM. That's how actual memories work. You can access any of them equally quickly. So that's memory. And so what we want to do is when we say an array, we want this to be a consecutive chunk of memory. Let me get a color. So let's say I have an array of size four, and it lives here. I can't spell, but I can't count. I think that's four. So the array starts here, and it ends over here. It's size four. And it's consecutive, which means if I want to access the array at position at index i, then this is the same thing as accessing the memory array at position wherever the array starts, which I'll call the address of the array. And Python, this is the ID of array, plus i. This is just simple offset arithmetic. If I want to know the zeroth item of the array, it's right here, where it starts. The first item is one after that. So as long as I store my array consecutively in memory, I can access the array in constant time. I can do get at and set at as quickly as I can randomly access the memory and get a value, or set a value, which we're assuming is constant time. So array access is constant time. This is what allows the static array to actually solve this problem in constant time per get at and set at operation. So this may seem simple, but we're really going to need this model and really rely on this model increasingly as we get to more interesting data structures. This is the first time we're actually needing it. Let's see. Length is also constant time. We're just going to store that number n along with its address. And build is going to take linear time. It will take linear time. Pretty straightforward. I guess one thing here, when defining build, I need to introduce a little bit more of our model of computation, which is, how do you create an array in the beginning? I can do it in linear time, and that's just part of the model. So this is called the memory allocation model. There are a few possible choices here. But the cleanest one is just to assume that you can allocate an array of size n in theta n time. So it takes linear time to make an array of size n. You could imagine this being constant. It doesn't really matter much, but it does take work. And in particular, if you just allocate some chunk of memory, you have no idea whether it's initialized. So initializing that array to zeros will cost linear time. It won't really matter constant versus linear. But a nice side effect of this model is that space, if you're just allocating arrays, the amount of space you use is mostly the amount of time you use. Or I guess, big O of that. So that's a nice feature. It's pretty weird if you imagine. It's unrealistic to imagine you can allocate an array that's infinite size and then just use a few items out of it. That won't give you a good data structure. So we'll assume it costs to allocate memory. And okay, great. We solved the static sequence problem. Very simple kind of boring. These are optimal running times. Now, let's make it interesting. Make sure I didn't miss anything. And talk about, oh. There is one thing I want to talk about in the word RAM. A side effect of this assumption that array access should take constant time and that accessing these positions in my memory should take constant time is that we need to assume W is at least log n or so. W, remember, is the machine word size in real computers. This is currently 64 or 256 and some bizarre instructions. But we don't usually think of the machine as getting bigger over time. But you should think of the machine as getting bigger over time. This is a statement that says the word side has to grow with n. It has to grow at least as fast as log n. Why do I say that? Because if I have n things that I'm dealing with, n here is the problem size. Maybe it's the RAM trying to store or whatever. If I'm having to deal with n things in my memory, at the very least, I need to be able to address them. I should be able to say, give me the i-th one and represent that number i in a word. Otherwise, because the machine is designed to only work with W bit words in constant time, if I want to be able to access the i-th word in constant time, I need a word size that's at least log n just to address the n things in my input. So this is a totally reasonable assumption. It may seem weird because you think of a real machine as having constant size. But you know, a real machine has constant size RAM also. My machine has 24 gigs of RAM or whatever. That laptop has 8. But you don't think of that as changing over time. But of course, if you wanted to process a larger input, you would buy more RAM. So eventually, when our ends get really, really big, we're going to have to increase W just so we can address that RAM. That's the intuition here. But this is a way to bridge reality, which are fixed machines, with theory in algorithms we care about scalability for very large n. We want to know what that growth function is and ignore the lead constant factor. That's what asymptotic notation is all about. And for that, we need a notion of word size also changing in this asymptotic way. All right. That will be more important next week when we talk about hashing and why hashing is a reasonable thing to do. But let's move on to dynamic sequences, which is where things get interesting. So I have the update here. So we start with static sequences. So all of these operations are still something we want to support in the dynamic sequence. But we add two dynamic operations, somewhat controversial operations. They're exciting. I want to be able to insert in the middle of my sequence. And I want to be able to delete from the middle of my sequence. So here's my sequence, which I'm going to think of in a picture, I'm going to draw it as an array. But it's stored. However, it's stored. We don't know. This is an interface, not an implementation. So let's say I insert at position 2. So position 2 is here. So I come in with my new x. And I would like x to be the new x2. But I don't want to lose any information. If I did set at 2, then I would erase this and replace it with x. But I want to do insert at, which means all of these guys are over by 1 in terms of their indices. So then I would get this picture that's 1 bigger. And now I've got the new x. I've got what was the old x2, which I hesitate to call x2 because that's its old name, not its new name. I'm going to draw arrows to say, these guys get copied over. These ones are definitely unchanged. x2, which I'll call x2 prime, is x. This is x3 prime, x4 prime, so on. I want to be careful here. And of course, the new n prime is n plus 1. I want to be careful about the labeling. Because the key, what makes insert at interesting, is that later, when I call get at, it's with the new indexing. So previously, if I did get at at 2, I would get this value. Afterwards, if I did get at at 2, I get the new value. If I did get at at 3 down here, I would get the value that used to be x2. That's maybe hard to track. But this is a conceptually very useful thing to do, especially when you're inserting or deleting at the ends. So we're going to define, in particular, insert and delete first and last. These are sometimes given, if you have an insert, if it has an x, if you do a delete, it has no argument. So this means insert at the beginning of the array, which would be like adding it here. And insert last means adding it on here. So insert last doesn't change the indices of any of the old items. That's a nice feature of insert last. Insert first changes all of them. They all get incremented by one. And we're also interested in the similar things here. We could do get first or last or set first or last. Which are the obvious special cases of get at and set at. These special cases are particularly interesting in an algorithms context. If you were a mathematician, you would say, why do I even bother this? This is just shorthand for a particular call to get or set. But what makes it interesting, from a data structure's perspective, is that we care about algorithms for supporting these operations. And maybe the algorithm for supporting get first or set first or in particular insert first or insert last might be more efficient. Maybe we can solve this problem better than we can solve insert at. So while ideally we can solve the entire dynamics sequence interface constant time per operation, that's not actually possible. You can prove that. But special cases of it where we're just inserting and deleting from the end say, we can do that. So that's why it's interesting to introduce special cases that we care about. Cool. That's the definition of the dynamics sequence interface. Now we're going to actually solve it. So our first data structure for this is called linked lists. You've probably seen linked lists before at some point. But the main new part here is we're going to actually analyze them and see how efficiently they implement all these operations we might care about. So first review. What is a linked list? We store our items in a bunch of nodes. Each node has an item in it and the next field. So you can think of these as class objects with two class variables, the item and the next pointer. And we assemble those into this kind of structure where we store in the item fields we're going to store the actual values that we want to represent in our sequence x0 through xn minus 1 in order. And then we're going to use the next pointers to link these all together in that order. So the next pointers are what actually give us the order. And in addition, we're going to keep track of what's called the head of the list. So the data structure is going to be represented by head. If you wanted to, you could also store length. So this could be the data structure itself. And it's pointing to all of these types of data structures. Notice we've just seen an array-based data structure, which is just a static array. And we've seen a pointer-based data structure. And we're relying on the fact that pointers can be stored in a single word, which means we can dereference them. We can see what's on the other side of the pointer in constant time in our word RAM model. In reality, each of these nodes is stored somewhere in the array of the computer. So maybe each one is two words long. So maybe one node is the first node is here, maybe the second node is here, the third node is here, there's some arbitrary order. We're using this fact that we can allocate an array of size n in linear time. In this case, we're going to have arrays of size 2. So we can just say, oh, please give me a new array of size 2. And that will make us one of these nodes. And then we're storing pointers. Pointers are just indices into the giant memory array. They're just, what is the address of this little array? If you've ever wondered how pointers are implemented, they're just numbers that say where in memory is this thing over here. And in memory, they're an arbitrary order. This is really nice because it's easy to manipulate the order of a linked list without actually physically moving nodes around, whereas arrays are problematic. Maybe it's worth mentioning, let's start analyzing things. So we care about these dynamic sequence operations. And we could try to apply it to the static array data structure, we could try to implement these operations in a static array. It's possible, just not going to be very good. And we can try to implement it with linked lists. And it's also not going to be that great. Let's go over here. Our goal is the next data structure, which is dynamic arrays. But linked lists and static arrays each have their advantages. So let's first analyze dynamic sequence operations, first on a static array. And then on a linked list. So in a static array, I think you all see, if I try to insert at the beginning of a static array, that's kind of the worst case. If I insert first, then everybody has to shift over. If I'm going to maintain this invariant, that the i-th item in the array represents, I didn't write it anywhere here. Maybe here. Static array, we're going to maintain this invariant that a of i represents xi. If I want to maintain that at all times, when I insert a new thing in the front, because the indices of all the previous items change, I have to spend time to copy those over. You can do it in linear time, but no better. So static array, insert and delete anywhere, cost theta n time, actually for two reasons. Number one is that if we're near the front, then we have to do shifting. What about insert or delete the last element of an array? Is that any easier? If I insert the very last element, none of the indices change. I'm just adding a new element. So I don't have to do shifting. So can I do insert and delete last in constant time in a static array? No, because the size is constant. So our model is that, number allocation model, is that we can allocate a static array of size n. But it's just a size n. I can't just say, please make it bigger by one. I need space to store this extra element. And if you think about where things are in memory, when you call to this memory allocator, which is part of your operating system, you say, please give me a chunk of memory, it's going to place them in various places in memory, and some of them might be next to each other. So if I try to grow this array by one, there might already be something there, and that's not possible without first shifting. So even though in the array, I don't have to do any shifting, in memory, I might have to do shifting. And that's outside the model. So we're going to stick to this model, just you can allocate memory. You can also de-allocate memory, just to keep space, usage small. But the only way to get more space is to ask for a new array, and that new array won't be contiguous to your old one. So question? What does a dynamic array will be the next topic? So maybe we'll come back to that. So in a static array, you're just not allowed to make it bigger. And so you have to allocate a new array, which we say takes linear time. Even if allocating the new array didn't take linear time, you have to copy all the elements over. From the older array to the new one, then you can throw away the old one. So just the copying from an array of size n to an array of size n plus one, that will take linear time. So static arrays are really bad for dynamic operations. No surprise, but you could do them. That's static array. Now linked lists are going to be almost the opposite. Well, almost. So if we store the length, OK, we can compute the length of the array very quickly, we can insert and delete at the front really efficiently. If I want to add a new item at the beginning as a new first item, then what do I do? I allocate a new node, which I'll call x. So this is insert first of x. I have a new array of size 2. I'm going to change, let me do it in red. I'm going to change this head pointer. Maybe I should do that later. So I'm going to set the next pointer here to this one. And then I'm going to change this head pointer to point to here. And boom, now I've got a linked list. Again, we don't know anything about the order and memory of these lists. We just care about the order that's represented implicitly by following the next pointers repeatedly. So now I've got a new list that has x in front, and then x0, and then x1, and so on. So insert and delete first, at least, are really efficient. We won't get much more than that. Now linked list, insert and delete first are constant time. So that's cool. However, everything else is going to be slow. If I want to get the 10th item in a linked list, I have to follow these pointers 10 times. I go 0, 0, 1, 2, 3, and so on. Follow 10 next pointers, and I'll get the 10th item. Accessing the item is going to take order i time. So get and set at need i time, which in the worst case is theta n. So we have sort of complementary data structures here. On the one hand, a static array can do constant time get at set at. So it's very fast at the random access aspect because it's an array. Link lists are very bad at random access, but they're better at being dynamic. We can insert and delete at the beginning, at least, in constant time. Now if we want to actually insert and delete at a particular position, that's still hard because we have to walk to that position. Even inserting and deleting at the end of the list is hard, although that's fixable. And maybe I'll leave that for problem session or problem set, but an easy, so here's a small puzzle. Suppose you wanted to solve, get last efficiently in a linked list. How would you solve that in constant time? Yeah, doubly linked list is a good idea, but actually not the right answer. That's an answer to the next question I might ask. Yeah. Postor pointer to the last element. That's all we need here. And often a doubly linked list has this. We usually call this the tail, head and tail. And if we always just store a pointer to the last list, this is what we call data structure augmentation, where we add some extra information to the data structure and we have to keep it up to date all the time. So if we do an insert last or something, insert last also becomes easy because I can just add a new node here and update the pointer here. Delete last is trick here, that's where you need a doubly linked list. But whenever I add something to the end of this list, I have to update the tail pointer also. As long as I maintain this, now suddenly get last as fast in constant time. So linked lists are great if you're working on the ends even dynamically. Arrays are great if you're doing random access and nothing dynamic, nothing adding or deleting at the ends or in the middle. Okay, so our final goal for today is to get sort of the best of both worlds with dynamic arrays. We're going to try to get all the good running times of linked lists and all the good running times of static arrays. We won't get quite all of them, but most of them. And in some sense, another way to describe what these introductory lectures are about is telling you about how Python is implemented. So what we're going to talk about next, dynamic arrays. I've alluded to many times, but these are what Python calls lists. So you don't have to implement a dynamic array by hand because it's already built into many fancy new languages for free because they're so darn useful. This lecture is about how these are actually implemented and why they're efficient. And in restation notes, you'll see how to actually implement them if all you had were static arrays. But luckily, we have dynamic arrays, so we don't have to actually implement them. But inside the Python interpreter, this is exactly what's happening. So the idea is to relax the constraint, or the invariant, whatever, that the size of the array we use equals n, which is the number of items in the sequence. Remember, in the sequence problem, we're supposed to represent n items. With a static array, we allocated an array of size exactly n. So let's relax that. Let's not make it exactly n. Let's make it roughly n. How roughly? You can think about it for a while. But from an algorithm's perspective, usually when we say roughly, we mean throwaway constant factors. And that turns out to be the right answer here. It's not always the right answer. But we're going to enforce that the size of the array is theta n, probably also greater than or equal to n. 0.5 n would not be very helpful, so it's going to be at least n. And it's going to be at most some constant times n. 2n, 10n, 1.1 times n. Any of these constants will work. I'm going to use 2n here, but there are lots of options. And now, things almost work for free. There's going to be one subtlety here. And I'm going to focus on, we're still going to maintain that the i-th item of the array is represents xi. So this data structure, let me draw a picture. So we've got an array of some size. The first few items are used to store the sequence, but then there's going to be some blank ones at the end. Maybe we'll keep track of this. So the data structure itself is going to have an array, and it's going to have a length. Something like this. So we're also going to keep track of the length. So we know that the first length items are where their data is, and the remainder are just our meaningless. So now, if I want to go and do an insert last, what do I do? I just go to a of length and set it to x, and then I increment length. Boom. Easy. Constant time. Yeah. How do you know you have enough room? Indeed, I don't. This was an incorrect algorithm. But it's usually correct. As long as I have extra space, this is all I need to do for insert last. But I'm also going to store the size of the array. This is the actual, this whole thing is size, and this part is length. So length is always going to be less than or equal to size. So there's a problem. If length equals size, then I don't have any space. Just add to the end, unless n equals size. I'm using n and length for the same thing. So length here is same as n. That's our actual number of things we're trying to represent, and size, this is great. This is the interface size. This is what we're trying to represent, and this is the representation size. This is the size of my array. These are the number of items I'm trying to store in that array. This is the interface versus data structure. Here's the interface. Here's the data structure. Cool. What do I do in the case when n equals size? I'm going to have to make my array bigger. This should sound just like static arrays. We made our array bigger every time we inserted, and that was this linear cost of allocation. We're going to do that sometimes. So static arrays, we had to do it every single time because size equaled n. Now we have some flexibility. We're only going to do it sometimes. It's like cookies or sometimes food, apparently, according to modern cookie monster. I don't understand, but if n equals size, we're going to allocate a new array of size. Bigger. Bigger. I like it. Greater than size. How much bigger? Twice. Five things. Size plus five. Come on, Jason. Trolling me. There are a couple of natural traces here. One is a constant factor larger. You could use 1.1 or 1.01, or two, or five, or ten. The all work. Or you could use Jason's trolling answer of size plus a constant, like five. Why is this bad? Yeah. You'll have to do it again. You'll have to resize frequently. When? Five steps later. So in the original static array, we were reallocating every single time. That's like n plus one. We do n plus five that really doesn't change things if we ignore constant factors. Now we'll have to spend linear time every five steps instead of linear time every one step. That's still linear time per operation. We're changing the constant factor. Whereas two times size, well, now we have to think a little bit harder. Let's just think about the case where we're inserting at the end of an array. So let's say we do n insert last from an empty array. When do we resize? Well, at the beginning, I guess I didn't say what we do for an empty array. Let's say size equals one. So we can insert one item for free. As soon as we insert the second item, then we have to resize. That seems bad. Immediately we have to resize. Then we insert the third item. Okay, now let's draw a picture. So we start with one item. We fill it up. Then we grow to size two. Because that's twice one. Then we fill it up. Immediately we have to resize again. But now we get start to get some benefit. Now we have size four. And so we can insert two items before we have to resize. Now we're size eight. And we get to insert four items before we refill. So this is going to resize. And again, resize is our expensive, both because we have to pay to allocate the new array. I drew it is just extending it. But in fact, we're creating a whole new array. Then we have to copy all of the items over. So there's the allocation cost and then the copying cost. So it's linear either way. Now resize at n equals one, two, four, eight, sixteen. You know this sequence? All the powers of two. Because we're doubling. That is exactly powers of two. So we pay a linear cost. So this resize cost, the allocation and the copying, is going to be, it's linear each time. So it's one plus two plus four plus eight plus sixteen. Okay, really I should write this as some from i equals one to roughly log n. A lot of base two of n is lg of two to the i. Okay, if you want a terminus here, it's roughly n. It's actually the next, the previous power of two of n or something. Okay, but that won't matter. That'll just affect things by a constant factor. What is the sum of two to the i? This is a geometric series. Do I know the answer? Two to the top limit plus one minus one. Yeah, so this is the identity. Some of two to the i from i equals one to k is two to the k plus one plus one minus one. So the plus one's upstairs. And easy way to remember this is if you think in binary, as we all should, we're computer scientists, two to the i means you set the i-th bit to one. So here's a bit string. This is the i-th bit. This is two to the i. Zero's down here. If I sum them all up, what that means is I'm putting ones here. Okay, and if you think about what this means, this is up to k from zero. Sorry, I should do zero to be proper. So that's the left-hand side. The right-hand side is two to the k plus one, which is a one here, and the rest zero's. So if you know your binary arithmetic, if you add one to this, you get this. Or if you subtract one from this, you get this. Okay, this is why this identity holds. Or the higher level thing is to say, oh, this is a geometric series. So I know, you should know this, telling you now, geometric series are dominated by the last term, the biggest term. If you have any series, you can identify as geometric, which means it's growing at least exponentially. Then in terms of theta notation, you can just look at the last term and put a theta around it, and you're done. So this is theta the last term, like two to the log n, which is theta m. Cool. Linear time. Linear time for all of my operations. I'm doing n operations here, and I spend linear total time to do all the resizing. That's good. That's like constant each kind of. The kind of is an important notion, which we call amortization. So I want to say an operation takes t of n amortized time. If, let's say, any k of those operations take at most k times t of n. This is a little bit sloppy, but be good enough. So the idea is here, if this works for n, or k, to do n operations from an empty array here takes linear time, which means I would call this constant amortized. Amortized means a particular kind of averaging, averaging over the sequence of operations. So while individual operations will be expensive, one near the end, when I have to resize the array, it's going to take linear time just for that one operation. Most of the operations are cheap. Most of them are constant. So I can think of charging that high cost to all of the other operations that made it happen. And so this is averaging over the operation sequence. Every insert last over there only takes constant time on average over the sequence of operations that we do. And so it's almost constant. It's not quite as good as constant worst case, but it's almost as good. And it's as good as you could hope to do in this dynamic array allocation model. Let me put this into a table. And you'll find these in the lecture notes also. So we have on the top the main operations of the sequence interface, which we will revisit in lecture seven. We'll see some other data structures for this. Get at and set at in the first column, insert and delete first, insert and delete last, insert and delete at an arbitrary position. So we've seen three data structures. Now arrays were really good at get at set at. They took constant time. That's the blue one. We're emitting the theta's here. All the other operations took linear time no matter where they were. Link lists were really good at insert and delete first. They took constant time, but everything else took linear time in the worst case. These new dynamic arrays achieve get at and set at in constant time because they maintain this invariant here that a of i equals x i. So we can still do gets and set at quickly. And we also just showed that insert last is constant amortized. Delete last, you don't have to resize the array. You could just decrease length and boom, you've deleted the last item. It's not so satisfying because if you insert n items and then delete n items, you'll still have an array of size theta n, even though your current value of n is zero. You can get around that with a little bit more trickery, which are described in lecture notes. But it's beyond the, we're only going to do very simple amortized analysis in this class to prove that that algorithm is also constant amortized, which it is. You'll see in 046 or you can find it in the CLRS book. That's it for today.